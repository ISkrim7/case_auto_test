#!/usr/bin/env python# -*- coding:utf-8 -*-# @Time : 2025/2/27# @Author : cyq# @File : locustClient# @Software: PyCharmfrom typing import List, Dictfrom dotenv import load_dotenvfrom pydantic import BaseModel, Fieldload_dotenv()from locust.util.rounding import proper_roundimport anyiofrom utils.locustUser import create_dynamic_user_classfrom interface.starter import APIStarterfrom utils import log, GenerateToolsfrom locust.runners import Runner, LocalRunnerimport geventfrom locust import eventsfrom locust.env import Environmentfrom locust.log import setup_loggingfrom app.schema.interface import InterfaceApiSchema, PerfSchemasetup_logging("DEBUG", None)class LocustClient:    def __init__(self):        self._runner = {}    def start_locust(self,                     api: InterfaceApiSchema,                     perf_api_name: str,                     setting: PerfSchema,                     io=None):        """        api InterfaceApiSchema 接口信息        perf_api_name 接口名        setting 性能设置        """        log.debug(f"开始执行性能测试：{perf_api_name}")        log.debug(f"API：{api}")        log.debug(f"setting：{setting}")        # 动态创建 User 类        DynamicUserClass = create_dynamic_user_class(            api,            setting        )        # 创建Locust运行环境        env = Environment(user_classes=[DynamicUserClass], events=events, host=api.host)        # 本地运行        runner = env.create_local_runner()        self._runner[perf_api_name] = runner  # 保存runner实例        env.events.init.fire(environment=env, runner=runner)  # 触发初始化事件        # 启动一个协程用于记录统计数据        gevent.spawn(_my_stats_history, env.runner, io)        # 开始性能测试        runner.start(setting.perf_user, spawn_rate=setting.perf_spawn_rate)        # 在指定的duration秒后停止测试        gevent.spawn_later(setting.perf_duration_per_minute, runner.quit)        # 等待所有协程完成        runner.greenlet.join()        # 获取历史统计数据        history = runner.stats.history        log.debug(f"user:{runner.user_count}")        log.debug(f"请求数量：{runner.stats.num_requests}")        log.debug(f"失败数量：{runner.stats.num_failures}")        log.debug(f"请求total_response_time：{runner.stats.total.avg_response_time}")        log.debug(f"请求max_response_time：{runner.stats.total.max_response_time}")        log.debug(f"请求min_response_time：{runner.stats.total.min_response_time}")        log.debug(f"请求avg_response_time：{runner.stats.total.avg_response_time}")        log.debug(f"请求total_rps：{runner.stats.total.total_rps}")        data = anyio.run(push_data, runner, io)    def stop(self, taskId: str, io: APIStarter):        runner: LocalRunner = self._runner.get(taskId)        if runner:            anyio.run(push_data, runner, io)            runner.quit()    @property    def users(self):        return self._runnerdef _my_stats_history(runner: Runner, io: APIStarter) -> None:    """Save current stats info to history for charts of report."""    while True:        if not runner.stats.total.use_response_times_cache:            break        if runner.state != "ready" and runner.state != "stopped":            _my_update_stats_history(runner, io)        # 3秒一发送socket 信息        gevent.sleep(3)def _my_update_stats_history(runner: Runner, io: APIStarter) -> None:    stats = runner.stats    timestamp = GenerateTools.getTime(4)    current_response_time_percentiles = {        f"response_time_percentile_{str(percentile).replace('.', '')}": [            timestamp,            stats.total.get_current_response_time_percentile(percentile) or 0,        ]        for percentile in [0.5, 0.95]    }    data = anyio.run(push_data, runner, io, current_response_time_percentiles)    stats.history.append(data)async def push_data(runner: Runner, io: APIStarter, current_response_time_percentiles: dict = None):    stats = runner.stats    timestamp = GenerateTools.getTime(4)    data = {        "status": str(runner.state).upper(),        "max_response": proper_round(stats.total.max_response_time, digits=2),        "request_num": stats.num_requests,        "request_fail_num": stats.num_failures,        "total_rps": proper_round(stats.total.total_rps, digits=2),        "user_count": runner.target_user_count,        "current_rps": [timestamp, proper_round(stats.total.current_rps, digits=2) or 0],        "current_fail_per_sec": [timestamp, stats.total.current_fail_per_sec or 0],        "total_avg_response_time": [timestamp, proper_round(stats.total.avg_response_time, digits=2)],        "cpu": runner.current_cpu_usage,    }    if current_response_time_percentiles:        data.update(current_response_time_percentiles)    await io.push(data)    return datalocust_client = LocustClient()